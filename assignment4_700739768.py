# -*- coding: utf-8 -*-
"""Assignment4_700739768.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cr2T6qSai6F1BDJaf7l07MYMnm6Cusho

# **Question 1**
Apply Linear Regression to the provided dataset using underlying steps.

a. Import the given “Salary_Data.csv”

b. Split the data in train_test partitions, such that 1/3 of the data is reserved as test subset.

c. Train and predict the model.

d. Calculate the mean_squared error

e. Visualize both train and test data using scatter plot.
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
# %matplotlib inline

#Importing the dataset and extracting the independent and Dependent Variables
salaries = pd.read_csv("Salary_Data.csv")
X = salaries.iloc[:, :-1].values
y = salaries.iloc[:,1].values

salaries.head()

#Data Visualization
#Building Correlation Matrix
sns.heatmap(salaries.corr())

#Splitting the dataset into training set and test set
from sklearn import preprocessing
from sklearn.model_selection import train_test_split, cross_validate

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 1/3, random_state = 0)

#Fitting multiple linear regression to the training set
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

#Predicitng the test set results
y_pred = regressor.predict(X_test)
print(y_pred)

#Evaluating the model Calculating the R squared value 
from sklearn.metrics import r2_score
r2_score(y_test, y_pred)

#Calculate the mean_squared error
from sklearn.metrics import mean_squared_error
mean_squared_error(y_test,y_pred)

#Visualize both train and test data using scatter plot.

#Training data visulaization
plt.rcParams.update({'figure.figsize':(6,5), 'figure.dpi':100})
plt.title('Training Data')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.scatter(X_train, y_train)
plt.show()

#Testing data visulaization using scatter plot
plt.rcParams.update({'figure.figsize':(6,5), 'figure.dpi':100})
plt.title('Testing Data')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.scatter(X_test, y_test)
plt.show()

"""#Question 2

Apply K means clustering in the dataset provided:

• Remove any null values by the mean.

• Use the elbow method to find a good number of clusters with the K-Means algorithm 

• Calculate the silhouette score for the above clustering
"""

#Import the libraries
from sklearn.cluster import KMeans
from sklearn import metrics
from scipy.spatial.distance import cdist
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="white", color_codes=True)
import warnings
warnings.filterwarnings("ignore")

dataset = pd.read_csv('K-Mean_Dataset.csv')
dataset.head()

#Remove any null values by the mean.
gfg = pd.DataFrame(dataset)
# using isnull() function and check for any null values
gfg.isnull()

dataset.describe()

dataset.isnull().sum()

#Data Analytics
dataset = dataset.iloc[:,1:]
dataset['MINIMUM_PAYMENTS'] = dataset['MINIMUM_PAYMENTS'].fillna(np.mean(dataset['MINIMUM_PAYMENTS']))
dataset['CREDIT_LIMIT'] = dataset['CREDIT_LIMIT'].fillna(np.mean(dataset['CREDIT_LIMIT']))
dataset.isnull().sum()

##determining K values using elbow method to know the number of clusters
wcss = []

for i in range(1,11):
     kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)
     kmeans.fit(X)
     wcss.append(kmeans.inertia_)

plt.plot(range(1,11),wcss)
plt.title('the elbow method')
plt.xlabel('Number of Clusters')
plt.ylabel('Wcss')
plt.show()

#detemining K values Using Distortion and Inertias methods
distortions = []
inertias = []
mapping1 = {}
mapping2 = {}
K = range(1, 11)
  
for k in K:
    # Building and fitting the model
    kmeanModel = KMeans(n_clusters=k).fit(X)
    kmeanModel.fit(X)
  
    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,
                                        'euclidean'), axis=1)) / X.shape[0])
    inertias.append(kmeanModel.inertia_)
  
    mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_,
                                   'euclidean'), axis=1)) / X.shape[0]
    mapping2[k] = kmeanModel.inertia_

for key, val in mapping1.items():
    print(f'{key} : {val}')

plt.plot(K, distortions, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Distortion')
plt.title('The Elbow Method using Distortion')
plt.show()

for key, val in mapping2.items():
    print(f'{key} : {val}')

plt.plot(K, inertias, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Inertia')
plt.title('The Elbow Method using Inertia')
plt.show()

"""Use the elbow method to find a good number of clusters with the K-Means algorithm K

To Determine the optimal number of clusters, we have to select the value of k at the “elbow” ie the point after which the distortion/inertia start decreasing in a linear fashion. Thus for the given data, we conclude that the optimal number of clusters for the data is 4.
"""

from sklearn.cluster import KMeans
nclusters = 4 # this is the k in kmeans
km = KMeans(n_clusters=nclusters)
km.fit(X)

# predict the cluster for each data point
y_cluster_kmeans = km.predict(X)
#Calculate the silhouette score for the above clustering
from sklearn import metrics
score = metrics.silhouette_score(X, y_cluster_kmeans)
print("the Silhouette Score", score)

"""Question 3:
Try feature scaling and then apply K-Means on the scaled features.

Did that improve the Silhouette score? If Yes, can you justify why
"""

#Feature Scalling the dataset
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_dataset = pd.DataFrame(scaler.fit_transform(dataset), columns=dataset.columns)
scaled_dataset.head()

##determining K values using elbow method to know the number of clusters
scaled_wcss = []

for i in range(1,11):
     kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)
     kmeans.fit(scaled_dataset)
     scaled_wcss.append(kmeans.inertia_)

plt.plot(range(1,11),scaled_wcss)
plt.title('the elbow method')
plt.xlabel('Number of Clusters')
plt.ylabel('Wcss')
plt.show()

scaled_clusters = 4 # this is the k in kmeans
km = KMeans(n_clusters=nclusters)
km.fit(scaled_dataset)

# predict the cluster for each data point
scaled_y_cluster_kmeans = km.predict(scaled_dataset)
#Calculate the silhouette score for the above clustering
score = metrics.silhouette_score(scaled_dataset, scaled_y_cluster_kmeans)
print("the Silhouette Score", score)

"""The reason for the Silhouette score low after feature scaling is - On Applying the feature scale, the values come under the same range hence model becomes difficult to predict values."""